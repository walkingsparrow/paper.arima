\documentclass[english,12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{float} \usepackage{graphicx}
\usepackage{esint} \usepackage{natbib} \usepackage{times,bm} \usepackage{babel}
\usepackage{amsmath}
%\usepackage{algorithm2e}
\usepackage[noend]{algpseudocode}
\usepackage{xspace}

\makeatletter
\linespread{1.25}

	% BEGIN Algorithms
	\theoremstyle{algorithm}
	\theorembodyfont{\upshape}
	\newtheorem{algorithm}{Algorithm}[section]

	\newlength{\alglabelwidth}
	\newcommand{\alginput}[1]{%
		\par\noindent%
		\settowidth{\alglabelwidth}{\emph{Output:}}%
		\makebox[\alglabelwidth][l]{\emph{Input:}} \begin{tabular}[t]{l} #1 \end{tabular}}
	\newcommand{\algoutput}[1]{%
		\par\noindent%
		\settowidth{\alglabelwidth}{\emph{Output:}}%
		\makebox[\alglabelwidth][l]{\emph{Output:}} \begin{tabular}[t]{l} #1 \end{tabular}}
	\newcommand{\algprecond}[1]{%
		\par\noindent\textit{Initialization/Precondition: #1}}

	\newcommand{\set}{\leftarrow}
	\DeclareMathOperator{\random}{random}
	\newcommand{\dist}{\ensuremath{\mathit{dist}}}
	\newcommand{\List}{\mathrm{List}}
	\newcommand{\Sample}{\mathit{Sample}}
	\algblockdefx[With]{With}{EndWith}%
		[1]{\textbf{with} #1 \textbf{do}}%
		[0]{End}
	\algnotext[With]{EndWith}
	% END Algorithms


\pagestyle{headings}

\begin{document}

\title{Implementation of Parallel Algorithms for ARIMA in Distributed
  Database Systems}

% \author{}

\maketitle

\begin{abstract}
    MADlib is an open-source library for scalable in-database analytics. We
    impemented ARIMA in MADlib's framework. The algorithms for fitting ARIMA
    model to time series are intrinsically sequential, because any calculation
    for a specific time $t$ depends on the result from the calculation for
    $t-1$.  This makes it difficult to parallelize the ARIMA model fitting. Our
    solution parallelizes the computation by splitting the data into $n$
    chunks. Since the model fitting involves multiple iterations of
    computations. We use the results from previous iteration as the initial
    values for each chunk. Thus the computation for each chunk of data does not
    need to wait for the results from previous chunk. Another technique is used
    to improve the performance of the implementation, which redistributes the
    original data so that each chunk can be completely loaded into memory.
\end{abstract}

\section{Introduction}

% Introduce to MADlib, and implement ARIMA in MADLib
MADlib \cite{madlib} is an open source library for scalable in-database
analytics.  It is created by the Predictive Analytics Team at Pivotal Inc.
MADlib provides data-parallel implementations of mathematical, statistical and
machine-learning methods for structured and unstructured data.

\section{What is MADlib?}

% More details on MADlib

MADlib is created by the Predictive Analytics Team at Pivotal Inc. (previously
Greenplum). It is an add-on package for Greenplum database or PostgreSQL
database. The package itself is open sourced.

Cohen et al. \cite{mad-skills} explained that "MAD" stands for "magnetic",
"agile", "deep", and "lib" stands for advanced (mathematical, statistical,
machine learning), parallel and scalable in-database functions.

The MADlib project was initiated in late 2010. Currently MADlib's latest
version is 1.2. By itself MADlib provides a pure SQL interface. An R front-end
named PivotalR \cite{pivotalr} is also created by the Predictive Analytics Team
at Pivotal Inc. A Python wrapper \cite{python-madlib} for MADlib is also
available.

MADlib can be installed and run on both Grenplum database and PostgreSQL\@. On
Greenplum database (GPDB) systems, MADlib utilizes the data parallel
funtionality. The calculation is done on multiple segments of GPDB in parallel,
and the results from the segments are summarized on the master node. In many
cases, multiple iterations of such calculations are needed. We use C++ code to
implement the part inside each iteration, which is computation intensive. We
use Python code to drive the iterations, which does not have high requirements
for the performance.

% explain more about the C++ abstraction layer

% more about Python layer

At the time of writing, MADlib has modules for linear regression, logistic
regression, multinomial logistic regression, elastic-net regularization for
linear and logistic regressions, all kinds of robust estimators for
regressions, marginal effects, k-means, association rules, cross validation,
linear systems, matrix factorization, LDA, data summary, correlation,
hypothesis tests, SVD and PCA, ARIMA etc and many other supporting functions.
A very detailed user documentation is available onlin at
\url{http://madlib.net}.

Here in this paper, we focus our implementation for ARIMA in MADlib.

\section{Implementation of ARIMA}

% A short summary for the next few subsections

\subsection{The Algorithm}

% List the LM algorithm

An ARIMA model is an auto-regressive integrated moving average model. An ARIMA
model is typically expressed in the form
\begin{equation}
(1 - \phi(B)) Y_t  = (1 + \theta(B)) Z_t,
\end{equation}
where $B$ is the backshift operator. The time $t$ is from $1$ to $N$.

ARIMA models involve the following variables:
\begin{enumerate}
   \item The lag difference $Y_{t}$, where  $Y_{t} = (1-B)^{d}(X_{t} - \mu)$.
    \item The values of the time series $X_t$.
    \item $p$, $q$, and $d$ are the parameters of the ARIMA model.
      $d$ is the differencing order, $p$ is the order of the AR
      operator, and $q$ is the order of the MA operator.
    \item The AR operator $\phi(B)$.
    \item The MA operator $\theta(B)$.
    \item The mean value $\mu$, which is always set to be zero for
      $d>0$ or need to be estimated.
    \item The error terms $Z_t$.
\end{enumerate}

The  auto regression operator models the prediction for the next
observation  as some linear combination of the previous observations.
More formally, an AR operator of order $p$ is defined as
\begin{align}
\phi(B) Y_t= \phi_1 Y_{t-1}   + \dots +  \phi_{p} Y_{t-p}
\end{align}

The moving average operator is similar, and it models the prediction
for the next observation as a linear combination of the errors in the
previous prediction errors.  More formally, the MA operator of order
$q$ is defined as
\begin{align}
\theta(B) Z_t =   \theta_{1} Z_{t-1} + \dots + \theta_{q} Z_{t-q}.
\end{align}

We assume that
\begin{equation}
\Pr(Z_t) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-Z^2_t/2 \sigma^2}, \quad t > 0
\end{equation}
and that  $Z_{-q+1} = Z_{-q+2} = \dots = Z_0 = Z_1 = \dots = Z_p =
0$. The initial values of $Y_t=X_t-\mu$ for $t=-p+1, -p+2, \dots,
0$ can be solved from the following linear equations
\begin{eqnarray}
\phi_1 Y_0 + \phi_2 Y_{-1} + \cdots + \phi_p Y_{-p+1} &=& Y_1 \nonumber\\
\phi_2 Y_0 + \cdots + \phi_p Y_{-p+2} &=& Y_2 - \phi_1 Y_1  \nonumber\\
&\vdots& \nonumber\\
\phi_{p-1} Y_0 + \phi_p Y_{-1} &=& Y_{p-1} - \phi_1 Y_{p-2} - \cdots -
\phi_{p-2} Y_1 \nonumber \\
\phi_p Y_0  &=& Y_p - \phi_1 Y_{p-1} - \cdots - \phi_{p-1} Y_{1} \label{eq:init_Y}
\end{eqnarray}

The likelihood function $L$ for $N$ values of $Z_t$  is then
\begin{equation}
L(\phi, \theta) = \prod_{t = 1}^N  \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-Z^2_t/2 \sigma^2}
\end{equation}
so the log likelihood function $l$ is
\begin{align}
l(\phi, \theta) &= \sum_{t = 1}^N \ln \left(\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-Z^2_t/2 \sigma^2}
 \right) \nonumber\\
 &=  \sum_{t = 1}^N  - \ln \left( \sqrt{2 \pi \sigma^2}\right)  -\frac{Z^2_t}{2 \sigma^2}\nonumber\\
&=  -\frac{N}{2} \ln \left( 2 \pi \sigma^2\right)  - \frac{1}{2
  \sigma^2} \sum_{t = 1}^N   Z^2_t\ \. \label{eq:loglikelihood}
\end{align}
Thus, finding the maximum likelihood is equivalent to solving the
optimization problem (known as the conditional least squares
formation)
\begin{align}
\min_{\theta, \phi} \sum_{t = 1}^N  Z^2_t.
\end{align}
The error term $Z_t$ can be computed iteratively as follows:
\begin{equation}
    Z_t = X_t - F_t(\phi, \theta, \mu) \label{eq:error-terms}
\end{equation}
where
\begin{align}
F_t(\phi, \theta, \mu) = \mu + \sum_{i=1}^p \phi_i (X_{t-i}-\mu) + \sum_{i=1}^q \theta_i Z_{t-i}
\end{align}

In mathematics and computing, the Levenberg-Marquardt algorithm (LMA),
also known as the damped least-squares (DLS) method, provides a
numerical solution to the problem of minimizing a function, generally
nonlinear, over a space of parameters of the function. These
minimization problems arise especially in least squares curve fitting
and nonlinear programming.

To understand the Levenberg-Marquardt algorithm, it helps to know the
gradient descent method and the Gauss-Newton method.  On many
``reasonable'' functions, the gradient descent method takes large
steps when the current iterate is distant from the true solution, but
is slow to converge an the current iterate nears the true solution.
The Gauss-Newton method is much faster for converging when the current
iterate is in the neighborhood of the true solution.  The
Levenberg-Marquardt algorithm tries to get the best of best worlds,
and combine the gradient descent step with Gauss-Newton step in a
weighted average.  For iterates far from the true solution, the step
favors the gradient descent step, but as the iterate approaches the
true solution, the Gauss-Newton step dominates.

Like other numeric minimization algorithms, LMA is an iterative
procedure.  To start a minimization, the user has to provide an
initial guess for the parameter vector, $p$, as well as some tuning
parameters $\tau, \epsilon_1, \epsilon_2, \epsilon_3,$ and $k_{max}$.
Let $Z(p)$ be the vector of calculated errors ($Z_t$'s) for the
parameter vector $p$, and let $J = (J_{1}, J_{2}, \dots, J_N)^T$
be a Jacobian matrix.

A proposed implementation is as follows:

\begin{algorithm}
\alginput{An initial guess for parameters $\vec{\phi}_0, \vec{\theta}_0, \mu_0$}
\algoutput{The parameters that maximize the likelihood $\vec{\phi}^*,
  \vec{\theta}^*, \mu^*$}
\begin{algorithmic}[1]
	\State $k \leftarrow 0$  \Comment{Iteration counter}
	\State $v \leftarrow 2$ \Comment{The change in the weighting factor.}
	\State $(\vec{\phi},\vec{\theta},\mu) \leftarrow (\vec{\phi}_0,\vec{\theta}_0,\mu_0)$ \Comment{Initialize parameter vector}
    \State Calculate $Z(\vec{\phi},\vec{\theta},\mu)$ with
    equation~\ref{eq:error-terms}.  \Comment{Vector of errors}
	\State $A \leftarrow J^T J$   \Comment{The  Gauss-Newton Hessian approximation}
	\State $u \leftarrow \tau * \max_i(A_{ii})$ \Comment{Weight of the gradient-descent step}
	\State $g \leftarrow J^T Z(\vec{\phi},\vec{\theta},\mu)$ \Comment{The gradient descent step.}
	\State $ \text{stop} \leftarrow (\|g\|_{\infty} \le \epsilon_1)$ \Comment{Termination Variable}
	\While{(not stop) and ($k < k_{max}$)}
		\State $k \leftarrow k + 1$
		\Repeat
			\State $\delta \leftarrow (A + u \times \text{diag}(A))^{-1} g$ \Comment{Calculate step direction}
			\If{$\| \delta \| \le \epsilon_2 \|
              (\vec{\phi},\vec{\theta},\mu) \|$} \Comment{Change in the parameters is too small to continue.}
				\State  $\text{stop} \leftarrow \text{true}$
			\Else
				\State $(\vec{\phi}_{new},\vec{\theta}_{new},\mu_{new}) \leftarrow (\vec{\phi},\vec{\theta},\mu) + \delta$ \Comment{Take a trial step in the new direction}
				\State $\rho \leftarrow (\| Z(\vec{\phi},\vec{\theta},\mu)\|^2 - \| Z(\vec{\phi}_{new},\vec{\theta}_{new},\mu_{new})\|^2 )/(\delta^T(u \delta + g))$ \Comment{Calculate improvement of trial step}
				\If{$\rho > 0$} \Comment{Trial step was good, proceed to next iteration}
					\State $(\vec{\phi},\vec{\theta},\mu) \leftarrow (\vec{\phi}_{new},\vec{\theta}_{new},\mu_{new})$ \Comment{Update variables}
                    \State Calculate $Z(\vec{\phi},\vec{\theta},\mu)$ with
                    equation~\ref{eq:error-terms}.
					\State $A \leftarrow J^T J$
					\State $g \leftarrow J^T Z(\vec{\phi},\vec{\theta},\mu)$
					\State $ \text{stop} \leftarrow (\|g\|_{\infty} \le \epsilon_1)$ or $(\| Z(\vec{\phi},\vec{\theta},\mu)^2 \| \le \epsilon_3)$  \Comment{Terminate if we are close to the solution.}
					\State $v \leftarrow 2$
					\State $u \rightarrow u * \max(1/3, 1 - (2\rho - 1)^3 )$
				\Else  \Comment{Trial step was bad, change weighting on the gradient decent step}
					\State $v \leftarrow 2 v$
					\State $u \leftarrow u v$
				\EndIf
			\EndIf
		\Until{(stop) or ($\rho > 0$) }
	\EndWhile
	\State $(\vec{\phi}^*,\vec{\theta}^*,\mu^*) \leftarrow (\vec{\phi},\vec{\theta},\mu)$
\end{algorithmic}
\label{alg:LM}
\end{algorithm}

Suggested values for the tuning parameters are $\epsilon_1 = \epsilon_2 =
\epsilon_3 = 10^{-15}, \tau = 10^{-3}$ and $k_{max} = 100$.

\subsection{The Problems in Parallelization}

% What are the difficulties that we are facing

\subsection{Our Solution}

% Our solution and how to improve the performance

\subsection{Performance}

% Good performance improvement, some examples

\subsection{Discussion}

% Can be applied to other algorithms

\section{Conclusion}

\bibliographystyle{abbrv}

\bibliography{pred}

\end{document}
